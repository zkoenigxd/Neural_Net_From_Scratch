{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97372eb4",
   "metadata": {},
   "source": [
    "# Nueral Network Mathmatical Basis\n",
    "### Zachary Koenig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a23441",
   "metadata": {},
   "source": [
    "Import statements\n",
    "###### Note - only pulling dataset splitting and sample dataset from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfdc1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b2b6cc",
   "metadata": {},
   "source": [
    "Load the dataset and flatten the images for easy processing\n",
    "\n",
    "Split into 80% training data, and reserve the remaining 20% for verification after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits() # Load the 8x8 digits dataset\n",
    "\n",
    "# The dataset contains 8x8 images and their corresponding digit labels.\n",
    "# X contains image data and y contains the target labels.\n",
    "IMAGES = digits.images      # Shape: (num_images, 8, 8)\n",
    "LABELS = digits.target      # Shape: (num_images, 10)\n",
    "\n",
    "# --- Data Preprocessing ---\n",
    "\n",
    "# 1. Flatten the images:\n",
    "#    Convert each 8x8 image into a 1D array of 64 pixels.\n",
    "num_images = IMAGES.shape[0]\n",
    "flattened_images = IMAGES.reshape(num_images, -1)   # Now shape is (num_images, 64)\n",
    "\n",
    "# 2. Normalize the inputs:\n",
    "#    The pixel values in the digits dataset are in the range 0-16.\n",
    "#    Dividing by 16 scales them to the range [0, 1].\n",
    "normalized_flat_images = flattened_images / 16.0\n",
    "\n",
    "# 3. One-Hot Encode the Labels:\n",
    "#    There are 10 classes (digits 0-9), so create one-hot encoded vectors.\n",
    "num_classes = 10\n",
    "labels_onehot = np.eye(num_classes)[LABELS]\n",
    "\n",
    "# --- Split into Training and Verification (Validation) Sets ---\n",
    "# For example, reserve 20% of the data for validation\n",
    "image_train_set, image_validation_set, labels_train_set, labels_validation_set = train_test_split(\n",
    "    normalized_flat_images, labels_onehot, test_size=0.2, random_state=4651\n",
    ")\n",
    "\n",
    "# Verify the shapes\n",
    "print(\"Total Dataset Size:\", digits.images.shape[0], 'images with', digits.target.shape[0], 'labels\\n')\n",
    "print(\"Training set shape:\")  # Should show ~80% of the samples\n",
    "print(image_train_set.shape[0], 'images with', image_train_set.shape[1], 'pixels each')\n",
    "print(labels_train_set.shape[0], 'labels with', labels_train_set.shape[1], 'categories each\\n')\n",
    "print(\"Validation set shape:\") # Should show ~20% of the samples\n",
    "print(image_validation_set.shape[0], 'images with', image_validation_set.shape[1], 'pixels each')\n",
    "print(labels_validation_set.shape[0], 'labels with', labels_validation_set.shape[1], 'categories each')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c74c88",
   "metadata": {},
   "source": [
    "To enable nonlinear function representation, we'll use the \"Sigmoid\" and \"reLU\" activation functions\n",
    "\n",
    "### Sigmoid Function:\n",
    "\n",
    "$\\sigma(x) = \\Large\\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "(where $x$ in this program is the weighted sum of the inputs plus the bias from the layer just processed)\n",
    "\n",
    "- Squashes any real input into the interval $(0,1)$.\n",
    "- Smooth and differentiable everywhere, which makes gradient‑based learning possible.\n",
    "\n",
    "### Sigmoid Derivative:\n",
    "$\\sigma'(x) = \\sigma(x)\\,\\bigl(1 - \\sigma(x)\\bigr)$\n",
    "\n",
    "(where $x$ in this program is the weighted sum of the inputs plus the bias at the layer from the forward pass)\n",
    "\n",
    "- Simple closed‑form in terms of $σ(x).$\n",
    "- Used during backprop to propagate error gradients through the nonlinearity.\n",
    "\n",
    "### ReLU Function:\n",
    "\n",
    "$\\mathrm{ReLU}(x) = \\max(0,\\,x)$\n",
    "\n",
    "### ReLU Derivative:\n",
    "\n",
    "$\n",
    "\\mathrm{ReLU}'(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } x > 0,\\\\\n",
    "0 & \\text{if } x \\le 0.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "##### Why ReLU Is Useful in Neural Nets\n",
    "\n",
    "1. **Sparsity of Activation**  \n",
    "   - Negative inputs become zero, so many neurons are “off,” which can improve representational efficiency.\n",
    "\n",
    "2. **Mitigation of Vanishing Gradients**  \n",
    "   - For \\(x>0\\), the derivative is 1, helping gradients propagate without shrinking to near zero.\n",
    "\n",
    "3. **Computational Simplicity**  \n",
    "   - Only a comparison and selection are needed $(\\max(0,x))$, making ReLU very fast to compute.\n",
    "\n",
    "### Softmax Function\n",
    "**Mathematical form:**  \n",
    "\n",
    "$\n",
    "\\mathrm{softmax}(\\mathbf{z})_i\n",
    "= \\Large\\frac{e^{z_i}}{\\sum_{k=1}^K e^{z_k}}\n",
    "$\n",
    "\n",
    "(where)\n",
    "\n",
    "### Safe Softmax Function\n",
    "**Mathematical form (for a vector $\\mathbf{z}\\in\\mathbb{R}^K$):**\n",
    "\n",
    "$\n",
    "\\mathrm{softmax}(\\mathbf{z})_i\n",
    "= \\Large\\frac{e^{z_i - \\max_j (z_j)}}\n",
    "       {\\sum_{k=1}^K e^{z_k - \\max_j (z_j)}}\n",
    "$\n",
    "\n",
    "\n",
    "##### Risks of Standard Softmax:\n",
    "- **Numerical overflow**  \n",
    "   - If any $z_i$ is large $(e.g.\\ \\ z_i > 700)$, then $e^{z_i}$ can overflow to $\\infty$.  \n",
    "   - Subtracting $max_j (z_j)$ ensures the largest exponent is $e^0 = 1$, keeping all values in a safe range.\n",
    "\n",
    "- **Numerical underflow**  \n",
    "   - If all $z_i$ are very negative, $e^{z_i}$ can underflow to $0$.  \n",
    "   - Centering by $max_j (z_j)$ prevents all exponentials from collapsing to $0$, preserving meaningful probability ratios.\n",
    "\n",
    "##### Benefit of Safe Softmax\n",
    "- Converts raw scores into a probability distribution over $K$ classes (they sum to 1).\n",
    "- Subtracting $max_j (z_j)​$  ensures numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Activation functions and their derivatives ---\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Compute the sigmoid activation.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    \"\"\"Compute the derivative of the sigmoid function.\"\"\"\n",
    "    sigmoid_x = sigmoid(x)\n",
    "    return sigmoid_x * (1 - sigmoid_x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Computes the softmax function.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # For numerical stability\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"Compute the ReLU activation.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    \"\"\"Compute the derivative of the ReLU activation.\"\"\"\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54b1159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "hidden_layer_size = 40 # number of nodes at teach of the two hidden layers\n",
    "learning_rate = 0.01 # how quickly weights and biases are adjusted\n",
    "epochs = 10_000 # number of passes the training loop makes on the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff59e1",
   "metadata": {},
   "source": [
    "# Forward pass\n",
    "---\n",
    "The input to the network is a flattened image; in this case, the 8x8 pixel image is transformed into a 1x64 array, where each pixel is normalized for improved numerical stability.\n",
    "\n",
    "The network has one hidden layer, which contains $n$ neurons.  Each neuron in the hidden layer has a weight, denoted as $\\mathbf{W_{\\text{input}}}$,​ and a bias, $\\mathbf{b_{\\text{input}}}$.\n",
    "\n",
    "Initially, the weights are randomized, but these will be updated as the network trains. Alternatively, they could be systematically set or loaded from previously saved training data.\n",
    "\n",
    "The first step involves multiplying each pixel value in the array by the corresponding weight of the first hidden layer neuron, and then summing those weighted values. After that, a bias term is added to the sum.\n",
    "\n",
    "This process is repeated for each neuron in the hidden layer. For our test, we’ve set the hidden layer to have 64 neurons.\n",
    "\n",
    "At this point, the process is similar to linear regression. However, to introduce non-linearity, we apply an activation function to each neuron's output in the hidden layer. In this program, we use the sigmoid and ReLU activation functions.\n",
    "\n",
    "Similarly, the activated outputs from the hidden layer are linearly combined to form the inputs to the output layer. There are two key differences in this step:\n",
    "1. The output layer consists of a number of neurons equal to the number of classification labels. In this case, the labels represent digits from 0 to 9, so there are 10 neurons in the output layer.\n",
    "2. The activation function for the output layer is the Softmax function, which converts the raw output into a probability distribution over the 10 possible labels.\n",
    "\n",
    "### Hidden Layer:\n",
    "\n",
    "First, a linear transformation is applied to the input data with the weights and biases of the input layer.\n",
    "\n",
    "$\\mathbf{Z_1} = \\mathbf{X} \\cdot \\mathbf{W_{\\text{input}}} + \\mathbf{b_{\\text{input}}}$\n",
    "\n",
    "where:\n",
    "\n",
    " - $\\mathbf{X}$ is the input data.\n",
    "\n",
    " - $\\mathbf{W_{\\text{input}}}$​ is the matrix of weights for the input layer.\n",
    "\n",
    " - $\\mathbf{b_{\\text{input}}}$​ is the bias for the input layer.\n",
    "\n",
    " - $\\mathbf{Z_1}$​ is the weighted sum of inputs.\n",
    "\n",
    "Then, an activation function $f$ (such as ReLU, sigmoid, etc.) is applied to the output of this linear transformation:\n",
    "\n",
    "$\\mathbf{A_1} = f(\\mathbf{Z_1})$\n",
    "\n",
    "where $\\mathbf{A_1}$​ is the activated output from the hidden layer.\n",
    "\n",
    "### Output Layer:\n",
    "\n",
    "A similar linear transformation is applied to the output from the hidden layer:\n",
    "\n",
    "$\\mathbf{Z_2} = \\mathbf{A_1} \\cdot \\mathbf{W_{\\text{hidden}}} + \\mathbf{b_{\\text{hidden}}}$\n",
    "\n",
    "where:\n",
    "\n",
    " - $\\mathbf{A_1}$ is the activation output from the hidden layer.\n",
    "\n",
    " - $\\mathbf{W_{\\text{hidden}}}$​ is the matrix of weights for the hidden layer.\n",
    "\n",
    " - $\\mathbf{b_{\\text{hidden}}}$​ is the bias for the hidden layer.\n",
    "\n",
    " - $\\mathbf{Z_2}$​ is the weighted sum of the activations from the hidden layer.\n",
    "\n",
    "Finally, the softmax activation is applied to the output $\\mathbf{Z_2}$​ to obtain the predicted probabilities:\n",
    "\n",
    "$\\mathbf{A_2} = \\text{softmax}(\\mathbf{Z_2})$\n",
    "\n",
    "where:\n",
    "\n",
    " - $\\mathbf{A_2}$ is the output of the network, representing the probability distribution over the output classes.\n",
    "\n",
    "# Backpropagation\n",
    "---\n",
    "The backward pass aims to update the weights and biases of the network by calculating the gradients of the loss with respect to each weight and bias, and then using these gradients to adjust the parameters.\n",
    "\n",
    "The process begins by calculating the gradients for the output layer and then propagating those gradients backward to the hidden layer.\n",
    "\n",
    "### Output Layer:\n",
    "First, the gradient of the loss with respect to the output layer is computed. This is done using the softmax cross-entropy derivative, which simplifies as:\n",
    "\n",
    "$\\mathbf{dZ_2} = \\mathbf{A_2} - \\mathbf{target}$\n",
    "\n",
    "where:\n",
    "\n",
    " - $\\mathbf{A_2}$​ is the output from the network (the predicted probabilities).\n",
    "\n",
    " - $\\mathbf{target}$ is the true class labels for the input data.\n",
    "\n",
    "\n",
    "Next, the gradients of the weights and biases of the output layer are computed:\n",
    "\n",
    "$\\mathbf{dW_2} = \\frac{1}{m} \\cdot \\mathbf{A_1}^T \\cdot \\mathbf{dZ_2}$​\n",
    "\n",
    "$\\mathbf{db_2} = \\frac{1}{m} \\cdot \\sum \\mathbf{dZ_2}$\n",
    "\n",
    "where:\n",
    "\n",
    " - $m$ is the number of training examples.\n",
    "\n",
    " - $\\mathbf{A_1}$​ is the activated output from the hidden layer (which acts as input to the output layer).\n",
    "\n",
    " - $\\mathbf{dW_2}$​ represents the gradient of the weights in the output layer.\n",
    "\n",
    " - $\\mathbf{db_2}$​ represents the gradient of the biases in the output layer.\n",
    "\n",
    "### Hidden Layer:\n",
    "Now, the gradients for the hidden layer are calculated by first computing the gradient of the loss with respect to the activations of the hidden layer:\n",
    "\n",
    "$\\mathbf{dA_1} = \\mathbf{dZ_2} \\cdot \\mathbf{W_{\\text{hidden}}}^T$\n",
    "\n",
    "where:\n",
    "\n",
    " - $\\mathbf{dZ_2}$ is the gradient from the output layer.\n",
    "\n",
    " - $\\mathbf{W_{\\text{hidden}}}$​ is the weight matrix for the hidden layer.\n",
    "\n",
    "Then, the gradient of the weighted sum of inputs to the hidden layer is computed:\n",
    "\n",
    "$\\mathbf{dZ_1} = \\mathbf{dA_1} \\cdot f'(\\mathbf{A_1})$\n",
    "\n",
    "where: \n",
    "\n",
    "- $f'(\\mathbf{A_1})$ is the derivative of the activation function applied to the hidden layer's output (e.g., sigmoid or ReLU).\n",
    "\n",
    "Next, the gradients for the weights and biases of the hidden layer are computed:\n",
    "\n",
    "$\\mathbf{dW_1} = \\frac{1}{m} \\cdot \\mathbf{data}^T \\cdot \\mathbf{dZ_1}$\n",
    "\n",
    "$\\mathbf{db_1} = \\frac{1}{m} \\cdot \\sum \\mathbf{dZ_1}$\n",
    "\n",
    "where:\n",
    "\n",
    " - $\\mathbf{data}$ is the input data.\n",
    "\n",
    " - $\\mathbf{dW_1}$​ represents the gradient of the weights in the hidden layer.\n",
    "\n",
    " - $\\mathbf{db_1}$​ represents the gradient of the biases in the hidden layer.\n",
    "\n",
    "\n",
    "### Update Weights and Biases:\n",
    "Finally, the weights and biases of both the hidden and output layers are updated using the computed gradients. This is done by calling the update_weights and update_biases functions, which adjust the weights and biases by a step proportional to the learning rate:\n",
    "\n",
    "$\\mathbf{W_1} \\leftarrow \\mathbf{W_1} - \\text{learning\\_rate} \\cdot \\mathbf{dW_1}$\n",
    "\n",
    "$\\mathbf{b_1} \\leftarrow \\mathbf{b_1} - \\text{learning\\_rate} \\cdot \\mathbf{db_1}$\n",
    "\n",
    "$\\mathbf{W_2} \\leftarrow \\mathbf{W_2} - \\text{learning\\_rate} \\cdot \\mathbf{dW_2}$\n",
    "\n",
    "$\\mathbf{b_2} \\leftarrow \\mathbf{b_2} - \\text{learning\\_rate} \\cdot \\mathbf{db_2}$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $\\mathbf{W_1}, \\mathbf{b_1}$​ are the weights and biases for the hidden layer.\n",
    "\n",
    " - $\\mathbf{W_2}, \\mathbf{b_2}$​ are the weights and biases for the output layer.\n",
    "\n",
    " - $\\text{learning\\_rate}$ is the step size used to update the weights and biases.\n",
    "\n",
    "Through this process, the network learns by adjusting its parameters to minimize the loss function over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49285982",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, save_name, input_layer_size, hidden_layer_sizes = [], output_layer_size = 10, activ_func = sigmoid, activ_func_der = sigmoid_deriv):\n",
    "\n",
    "        self.save_name = save_name\n",
    "\n",
    "        # Layer sizes\n",
    "        self.layer_sizes = [input_layer_size] + hidden_layer_sizes + [output_layer_size]\n",
    "\n",
    "        self.Zi = []\n",
    "        self.Ai = []\n",
    "        \n",
    "        self.activation_function = activ_func\n",
    "        self.activation_function_derivative = activ_func_der\n",
    "        \n",
    "        # Initialize weights & biases:\n",
    "        rng = np.random.default_rng()\n",
    "        self.layer_weights = []\n",
    "        self.layer_biases  = []\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            in_dim  = self.layer_sizes[i]\n",
    "            out_dim = self.layer_sizes[i+1]\n",
    "            # small random weights, shape (in_dim, out_dim)\n",
    "            W = rng.normal(0, 1, size=(in_dim, out_dim)) * 0.01\n",
    "            b = np.zeros((1, out_dim))\n",
    "            self.layer_weights.append(W)\n",
    "            self.layer_biases.append(b)\n",
    "        \n",
    "        # Containers to hold forward‐pass intermediates\n",
    "        self.Zi = [None] * (len(self.layer_sizes) - 1)\n",
    "        self.Ai = [None] * (len(self.layer_sizes) - 1)\n",
    "        \n",
    "    def forward(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute outputs; store Zs and As for backprop.\"\"\"\n",
    "        A = data\n",
    "        for idx, (W, b) in enumerate(zip(self.layer_weights, self.layer_biases)):\n",
    "            Z = A @ W + b                       # shape (m, out_dim)\n",
    "            # if this is the last layer, apply softmax; else apply hidden activation\n",
    "            if idx == len(self.layer_weights) - 1:\n",
    "                A = softmax(Z)\n",
    "            else:\n",
    "                A = self.activation_function(Z)\n",
    "            self.Zi[idx] = Z\n",
    "            self.Ai[idx] = A\n",
    "        return A\n",
    "    \n",
    "    def backward(self, data, target, learning_rate=0.01):\n",
    "        \"\"\"Generic backprop for any number of layers, using cross‐entropy + softmax.\"\"\"\n",
    "        m = data.shape[0]\n",
    "        L = len(self.layer_weights)\n",
    "\n",
    "        # 1) Start with dZ at the output layer: softmax cross‐entropy\n",
    "        dZ = self.Ai[-1] - target  # shape (m, layer_sizes[-1])\n",
    "\n",
    "        # 2) Backpropagate through each layer in reverse\n",
    "        for idx in reversed(range(L)):\n",
    "            A_prev = data if idx == 0 else self.Ai[idx-1]      # activation coming into this layer\n",
    "            W = self.layer_weights[idx]\n",
    "\n",
    "            # gradients\n",
    "            dW = (A_prev.T @ dZ) / m\n",
    "            db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "\n",
    "            # update\n",
    "            self.layer_weights[idx] -= learning_rate * dW\n",
    "            self.layer_biases[idx]  -= learning_rate * db\n",
    "\n",
    "            # prepare dZ for next iteration (unless idx == 0)\n",
    "            if idx > 0:\n",
    "                dA_prev = dZ @ W.T\n",
    "                dZ = dA_prev * self.activation_function_derivative(self.Zi[idx-1])\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"Make predictions using the trained network.\"\"\"\n",
    "        return np.argmax(self.forward(data), axis=1)\n",
    "    \n",
    "    def save_progress(self):\n",
    "        # Save the trained weights and biases to disk\n",
    "        for idx, (W, b) in enumerate(zip(self.layer_weights, self.layer_biases)):\n",
    "            np.save(f\"W{idx}_{self.save_name}.npy\", W)\n",
    "            np.save(f\"b{idx}_{self.save_name}.npy\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44627e0",
   "metadata": {},
   "source": [
    "# Cross‑Entropy Loss\n",
    "\n",
    "**Mathematical form:**  \n",
    "$\n",
    "\\mathcal{L} = -\\frac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^K y_{i,k}\\,\\log\\bigl(A^{[2]}_{i,k} + \\epsilon\\bigr)\n",
    "$\n",
    "\n",
    "where $A^{[2]}_{i,k}$ is the output from the network\n",
    "\n",
    "#### Why add a small constant $\\epsilon$ for numerical stability\n",
    "\n",
    "1. **Avoid $\\log(0)\\to -\\infty$**  \n",
    "   - If $A^{[2]}_{i,k}=0$, then $\\log(0)$ is undefined.  \n",
    "   - Adding $\\epsilon$ ensures $\\log\\bigl(A^{[2]}_{i,k} + \\epsilon\\bigr)$ remains finite.\n",
    "\n",
    "2. **Prevent NaNs and infinities in loss computation**  \n",
    "   - Keeps the computed loss a real number even when predictions are exactly zero.\n",
    "\n",
    "3. **Stable backpropagation**  \n",
    "   - Ensures gradients remain well‑defined, avoiding infinite or undefined gradient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(output):\n",
    "    # Compute Cross-Entropy Loss (adding a small constant for numerical stability)\n",
    "    return -np.mean(np.sum(labels_train_set * np.log(output + 1e-8), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726e221",
   "metadata": {},
   "source": [
    "Helper function to calculate the number of correctly predicted labels in the neural network\n",
    "\n",
    "Returns:\n",
    " - Accuracy of training data predictions\n",
    " - Accuracy of validation set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9189af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_check(nn, pred_output):\n",
    "    # Training Accuracy (using the training forward pass already computed)\n",
    "    train_preds = np.argmax(pred_output, axis=1)\n",
    "    true_train_labels = np.argmax(labels_train_set, axis=1)\n",
    "    train_accuracy = np.mean(train_preds == true_train_labels)\n",
    "\n",
    "    # Validation: perform a forward pass over the validation set.\n",
    "    validation_output = nn.forward(image_validation_set)\n",
    "    val_preds = np.argmax(validation_output, axis=1)\n",
    "    true_val_labels = np.argmax(labels_validation_set, axis=1)\n",
    "    val_accuracy = np.mean(val_preds == true_val_labels)\n",
    "    \n",
    "    return (train_accuracy, val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19284c4e",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "Start a timer to get time-to-train\n",
    "\n",
    "**Repeat until number of epochs is satisfactory:**\n",
    "- Feed-forward the image data (a set of flattened images)\n",
    "- calculate the loss (single value for tracking approx. progress, true loss calculated in backpropigation)\n",
    "- Back-propigate the difference between expected and predicted outputs\n",
    "- (Occationally save data to document training progress)\n",
    "\n",
    "Stop timer, document\n",
    "\n",
    "Then, save progress data to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(neural_network):\n",
    "    # --- Lists to Save Progress Data ---\n",
    "    progress_data = []  # Each element: (epoch, loss, training set accuracy, validation set accuracy)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        prediction_output = neural_network.forward(image_train_set)\n",
    "        \n",
    "        loss = calculate_loss(prediction_output)\n",
    "        \n",
    "        neural_network.backward(image_train_set, labels_train_set, learning_rate)\n",
    "\n",
    "        # During training, sample 100 times to compute and record training accuracy and loss\n",
    "        if epoch % max(1, epochs // 100) == 0:\n",
    "            train_acc, val_acc = accuracy_check(neural_network, prediction_output)\n",
    "            progress_data.append((epoch, loss, train_acc, val_acc))\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f'Epoch: {epoch} Loss: {loss:.3f} Training Accuracy: {train_acc:.3f}')\n",
    "            if train_acc >= 1:\n",
    "                break\n",
    "\n",
    "    total_training_time = time.time() - start_time\n",
    "    print(f\"Total training time: {total_training_time:.2f} seconds\")\n",
    "    neural_network.save_progress()\n",
    "    # Save progress data to CSV\n",
    "    csv_filename = 'training_progress_' + neural_network.save_name + '.csv'\n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"epoch\", \"loss\", \"train_accuracy\", \"val_accuracy\"])\n",
    "        for record in progress_data:\n",
    "            writer.writerow(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ea972",
   "metadata": {},
   "source": [
    "Print sample image from validation set with image classification prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_image(neural_network):\n",
    "    # --- Display and Test One Image ---\n",
    "    # Select a random image from the validation set\n",
    "    idx = np.random.randint(0, image_validation_set.shape[0])  # Fix: Correctly get the index\n",
    "    img = image_validation_set[idx].reshape(8, 8)\n",
    "    label = np.argmax(labels_validation_set[idx])\n",
    "\n",
    "    # Save the image array to a file.\n",
    "    np.save(\"test.npy\", img)\n",
    "\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f\"Training Image - Digit: {label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Load the test image.\n",
    "    test_image = np.load('test.npy')\n",
    "\n",
    "    # Flatten the image so that it has shape (1, 64)\n",
    "    flat_test_image = test_image.reshape(1, -1)\n",
    "\n",
    "    # Perform the forward pass to get the softmax output (probabilities)\n",
    "    prediction_probs = neural_network.forward(flat_test_image)\n",
    "\n",
    "    # Get the predicted class (index of the maximum probability)\n",
    "    predicted_class = np.argmax(prediction_probs)\n",
    "\n",
    "    # Calculate the confidence (probability of the predicted class)\n",
    "    probability = np.max(prediction_probs)\n",
    "\n",
    "    print(\"Predicted class:\", predicted_class)\n",
    "    print(\"Confidence: {:.2f}%\".format(probability * 100))  # Correct confidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868faec8",
   "metadata": {},
   "source": [
    "Print % correct predictions out of validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a3335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy(neural_network):\n",
    "    # --- Perform Predictions on the Validation Set Using the Loaded Weights ---\n",
    "    predictions = neural_network.predict(image_validation_set)  # Fix the spelling of 'validation'\n",
    "\n",
    "    # Calculate the accuracy of the model on the test set.\n",
    "    # Convert the one-hot encoded labels to integer labels.\n",
    "    true_labels = np.argmax(labels_validation_set, axis=1)\n",
    "\n",
    "    # Accuracy Calculation\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "    print(\"Test set accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcca0039",
   "metadata": {},
   "source": [
    "Plot histogram of number of accurate/inaccurate guesses per confidence interval (0 - no confidence, 1 - total confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e504b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confidence_comparison(probabilities, true_labels, bins=20, title = ''):\n",
    "    \"\"\"\n",
    "    Plots histograms comparing the confidence scores for correctly vs.\n",
    "    incorrectly classified samples.\n",
    "    \n",
    "    Parameters:\n",
    "        probabilities (np.ndarray): A 2D array of shape (n_samples, n_classes) \n",
    "            containing the predicted probabilities (e.g., outputs of softmax).\n",
    "        true_labels (np.ndarray): A 1D array of true labels (integers) of shape (n_samples,).\n",
    "            If your labels are in one-hot format, convert them using np.argmax.\n",
    "        bins (int): Number of bins for the histogram.\n",
    "    \"\"\"\n",
    "    # Compute predicted labels (choose the class with the highest probability).\n",
    "    predictions = np.argmax(probabilities, axis=1)\n",
    "    \n",
    "    # Compute the confidence for each prediction as the maximum probability.\n",
    "    confidences = np.max(probabilities, axis=1)\n",
    "\n",
    "    # Create consistent bin edges from 0 to 1.\n",
    "    bin_edges = np.linspace(0, 1, bins + 1)\n",
    "    \n",
    "    # Separate confidence scores based on whether the prediction was correct.\n",
    "    correct_confidences = confidences[predictions == true_labels]\n",
    "    incorrect_confidences = confidences[predictions != true_labels]\n",
    "    \n",
    "    # Plot histograms for the two groups.\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(correct_confidences, bins=bin_edges, alpha=0.7, label='Correct Predictions', color='green')\n",
    "    plt.hist(incorrect_confidences, bins=bin_edges, alpha=0.7, label='Incorrect Predictions', color='red')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.title(title + ' Confidence Comparison: Correct vs. Incorrect Predictions')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b165277",
   "metadata": {},
   "source": [
    "Plot loss, train accuracy, and verification accuracy as a function of number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17008c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_progress(csv_filepath, title = ''):\n",
    "    \"\"\"\n",
    "    Reads the CSV with training progress data (epoch, loss, training accuracy, validation accuracy)\n",
    "    and plots the loss and accuracy curves. Loss is plotted on the left y-axis, while accuracy (both training\n",
    "    and validation) is plotted on the right y-axis with training accuracy in red and validation accuracy in green.\n",
    "    \"\"\"\n",
    "    epochs_list = []\n",
    "    loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    # Read data from the CSV\n",
    "    with open(csv_filepath, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            epochs_list.append(float(row[\"epoch\"]))\n",
    "            loss_list.append(float(row[\"loss\"]))\n",
    "            train_acc_list.append(float(row[\"train_accuracy\"]))\n",
    "            val_acc_list.append(float(row[\"val_accuracy\"]))\n",
    "\n",
    "    # Create a new figure for each plot\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))  # New figure each time\n",
    "\n",
    "    # Plot Loss on the left y-axis\n",
    "    color_loss = 'tab:blue'\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss', color=color_loss)\n",
    "    ax1.plot(epochs_list, loss_list, marker='o', color=color_loss, label='Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor=color_loss)\n",
    "\n",
    "    # Create a second y-axis for the accuracies\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Accuracy', color='black')\n",
    "    # Set y-axis range from 0 to 1.05\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    ax2.plot(epochs_list, train_acc_list, marker='x', color='red', label='Train Accuracy')\n",
    "    ax2.plot(epochs_list, val_acc_list, marker='x', color='green', label='Validation Accuracy')\n",
    "    ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "    # Combine legends from both axes and display\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "\n",
    "    plt.title(title + ' Training Progress: Loss and Accuracy Every 100 Epochs')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773a8f6d",
   "metadata": {},
   "source": [
    "Run the following block to test Sigmoid vs reLU hidden layer performance on the created test set with identical hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_sig = NeuralNet(\"sig\", image_train_set.shape[1], [40, 20], labels_train_set.shape[1])\n",
    "neural_network_reLU = NeuralNet(\"reLU\", image_train_set.shape[1], [40, 20], labels_validation_set.shape[1], relu, relu_deriv)\n",
    "\n",
    "training_loop(neural_network_sig)\n",
    "test_single_image(neural_network_sig)\n",
    "print_accuracy(neural_network_sig)\n",
    "\n",
    "training_loop(neural_network_reLU)\n",
    "test_single_image(neural_network_reLU)\n",
    "print_accuracy(neural_network_reLU)\n",
    "\n",
    "# Get the predicted probabilities (outputs of softmax)\n",
    "predictions_output_sig = neural_network_sig.forward(image_validation_set)\n",
    "predictions_output_reLU = neural_network_reLU.forward(image_validation_set)\n",
    "\n",
    "# Plot the confidence comparison using the true labels\n",
    "true_labels = np.argmax(labels_validation_set, axis=1)  # Convert one-hot to integer labels\n",
    "plot_confidence_comparison(predictions_output_sig, true_labels, bins=50, title = 'Sigmoid')\n",
    "plot_confidence_comparison(predictions_output_reLU, true_labels, bins=50, title ='reLU')\n",
    "\n",
    "plot_training_progress('training_progress_' + neural_network_sig.save_name + '.csv', title = 'Sigmoid')\n",
    "plot_training_progress('training_progress_' + neural_network_reLU.save_name + '.csv', title = 'reLU')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
